{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For today's code challenge you will be reviewing yesterdays lecture material. Have fun!\n",
    "\n",
    "### if you get done early check out [these videos](https://www.3blue1brown.com/neural-networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGb0yyBtBCBD"
   },
   "source": [
    "# The Perceptron\n",
    "\n",
    "The first and simplest kind of neural network that we could talk about is the perceptron. A perceptron is just a single node or neuron of a neural network with nothing else. It can take any number of inputs and spit out an output. What a neuron does is it takes each of the input values, multplies each of them by a weight, sums all of these products up, and then passes the sum through what is called an \"activation function\" the result of which is the final value.\n",
    "\n",
    "I really like figure 2.1 found in this [pdf](http://www.uta.fi/sis/tie/neuro/index/Neurocomputing2.pdf) even though it doesn't have bias term represented there.\n",
    "\n",
    "![Figure 2.1](http://www.ryanleeallred.com/wp-content/uploads/2019/04/Screen-Shot-2019-04-01-at-2.34.58-AM.png)\n",
    "\n",
    "If we were to write what is happening in some verbose mathematical notation, it might look something like this:\n",
    "\n",
    "\\begin{align}\n",
    " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
    "\\end{align}\n",
    "\n",
    "Understanding what happens with a single neuron is important because this is the same pattern that will take place for all of our networks. \n",
    "\n",
    "When imagining a neural network I like to think about the arrows as representing the weights, like a wire that has a certain amount of resistance and only lets a certain amount of current through. And I like to think about the node itselef as containing the prescribed activation function that neuron will use to decide how much signal to pass onto the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kXZIF45EEuex"
   },
   "source": [
    "# Activation Functions (transfer functions)\n",
    "\n",
    "In Neural Networks, each node has an activation function. Each node in a given layer typically has the same activation function. These activation functions are the biggest piece of neural networks that have been inspired by actual biology. The activation function decides whether a cell \"fires\" or not. Sometimes it is said that the cell is \"activated\" or not. In Artificial Neural Networks activation functions decide how much signal to pass onto the next layer. This is why they are sometimes referred to as transfer functions because they determine how much signal is transferred to the next layer.\n",
    "\n",
    "## Common Activation Functions:\n",
    "\n",
    "![Activation Functions](http://www.snee.com/bobdc.blog/img/activationfunctions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9x3x5XgtD3i"
   },
   "source": [
    "# Implementing a Perceptron from scratch in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A47bcPUYYf8S"
   },
   "source": [
    "### Establish training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9Sj_AVzReca"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)\n",
    "\n",
    "inputs = np.array([\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1]\n",
    "])\n",
    "\n",
    "correct_outputs = [[0], [1], [1], [0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJYHTTThYlcj"
   },
   "source": [
    "### Sigmoid activation function and its derivative for updating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BXtmF6m1Ry2E"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "  sx = sigmoid(x)\n",
    "  return sx * (1 - sx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWyVzV-oUTC8"
   },
   "source": [
    "## Updating weights with derivative of sigmoid function:\n",
    "\n",
    "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iHpgMkpQX9HK"
   },
   "source": [
    "### Initialize random weights for our three inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0099616 ],\n",
       "       [ 0.21185521],\n",
       "       [-0.08502562]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = 2 * np.random.random((3,1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNhy_Qk2YBJO"
   },
   "source": [
    "### Calculate weighted sum of inputs and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08502562],\n",
       "       [ 0.13679119],\n",
       "       [-0.07506402],\n",
       "       [ 0.12682959]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_sum = np.dot(inputs,weights) # mulit inputs by the weights\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4z3LqLFaWMy"
   },
   "source": [
    "### Output the activated value for the end of 1 training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47875639],\n",
       "       [0.53414457],\n",
       "       [0.4812428 ],\n",
       "       [0.53166496]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activated_output = sigmoid(weighted_sum)\n",
    "activated_output \n",
    "# these are our predictions \n",
    "# they have a high amount of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_P2i3KEaiVC"
   },
   "source": [
    "### take difference of output and true values to calculate error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47875639],\n",
       "       [ 0.46585543],\n",
       "       [ 0.5187572 ],\n",
       "       [-0.53166496]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = (correct_outputs - activated_output )\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11308442],\n",
       "       [ 0.10853639],\n",
       "       [ 0.12246107],\n",
       "       [-0.12394888]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted = error * sigmoid_derivative(activated_output)\n",
    "adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tc928NEda0UE"
   },
   "source": [
    "### Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 21.86529342]\n",
      " [ -0.40553738]\n",
      " [-10.64724723]]\n",
      "[[2.37656881e-05]\n",
      " [9.99979854e-01]\n",
      " [9.99986570e-01]\n",
      " [1.58427726e-05]]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(300000):\n",
    "    weighted_sum = np.dot(inputs,weights)\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    error = (correct_outputs-activated_output)\n",
    "    adjusted_output = error * sigmoid_derivative(activated_output) # you should increase the weight by the corresponding adjusted_oputput\n",
    "    weights+= np.dot(inputs.T,adjusted_output)\n",
    "print(weights)\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tuesday's Challenge.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
